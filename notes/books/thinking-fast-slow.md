# Thinking Fast And Slow

Notes from the book [Thinking Fast and Slow](https://www.goodreads.com/book/show/11468377-thinking-fast-and-slow).

## Friendship and Collaboration

On the beginning of the book it caught my attention the author history
with one of his friends and main collaborators on all his research along
his life. They worked together for decades and they rarely agreed 100%
on anything, and yet the disagreements were productive. I find this kind
of balance intriguing, it is rare to find. So when he tried to describe why
it worked ou so well I paid some extra attention:

```
We were sufficiently similar to understand each other easily, and sufficiently
different to surprise each other.
```

It is really hard to hit that nice spot, when you do consider yourself lucky :-).

Off topic but I also found interesting how long walks to think things through
are essential to the author. I find it interesting mostly because I also depend
a lot on that and it always feels good to find kindred spirits.

There is also a degree of luck on how their research was done, the methodology
they choose was very efficient and produced great results, but the main reason
they choose it was because it was the most fun one, they were not trying
to come up with something great, they just did what was more fun.

It is a recurrent topic along the book how luck plays a huge role on most
achievements, both in research and industry. Of course you can't be lucky if
you are not doing anything, but just doing and even skill is not enough, you
need some luck too (that is why it is essential to keep trying, statistically
you are bound to some luck).

## The Two Systems

One of the first models presented on the book and the one more largely utilized
is the two systems model. Basically it splits our cognition/brain into two systems,
they are named 1 and 2, for simplicity (sometimes people refer to it as left/right
brain, or irrational/rational, but reality is more complex than that, so the
author just goes with 1/2).

System 1 is:

* Fast
* Parallel
* Intuitive (pattern recognitiion)
* Associative
* Creative (built on associativity)
* Seeks Causality / Coherence
* Story telling oriented

System 2 is:

* Slow
* Linear
* Logical
* Analytical
* Expensive

A lot of what we are on the conscient level is System 2, it is were language is.
But a lot of what makes us awesome, the magical associative machine, is System 1.
Most of the book explores biases and issues that arise from how we end up relying
way too much on System 1. It is quite fast and cheap, and it is right a lot of times,
but it can be remarkably wrong and it has no way to detect it is wrong.

One example is that System 1 craves causality and story telling. If a story makes sense
and it fits with your belief system, system 1 will buy it, even if the story
is completely disconected from reality and makes no sense at all outside
your belief system. If it matches what you believe it is confortable, and System 1
optimizes for confort.

Cognitive dissonance, the struggle of reassessing your world view, that is System 2.
It is expensive and it is unconfortable, hence most people don't do it very much.

This is pretty much the core from were a bunch of very interesting biases and issues
arise in our cognitive processes. In the end we are neither rational or irrational,
we are something else, an amalgamation of both interacting in useful ways.

It is important to remember were we came from. In nature resources are scarce.
That is why we are intrinsically lazy and use as little as possible from System 2.
For the first time in human history we actually have resources an time to focus
on System 2 activities. The life of our ancestors were quite different, and this
dictates how our brains are wired. And the best you can do is have more awareness
of this, but the biases/pitfalls are literally unavoidable, unless you are not human.

## Thinking is a phisical activity

When you engange into thinking, System 2 thinking, your whole body engages into it.
Research shows that your pupils dilate and your heart rate changes when you
start to engage into tasks that require a lot of system 2 thinking. It is a demanding
activity for your whole body, that is why we are pretty lazy about it.

## Ego Depletion

When you need to excert discipline and self control, an activity that is system 2,
you use a lot of energy. It is not as obvious as when you run and feel tired, but
it is there. Research showed how people after having to excert a lot of self control
gave up easier in a next challenge than people who were just starting.

This phenomenon is referred as **ego depletion**. When your ego is depleted you will
be much more prone to make decisions with your System 1, because you are literally
exausted mentally.

This also applies to when you have to do something that you don't want to do.
I immediately made a connection between this and meetings. I always found amazing
how meetings have the ability of completely fuck up my entire day, even when it is
"just" an hour or two. The fact that I need to force myself to do something
I don't want to and don't see the point causes burns through the ego, and then
you have the cost of the interruption itself. In the end it makes perfect sense
that one or two meetings can destroy a day. It seems exagerated, I always felt
more sensible than the normal, specially because Im usually the only one complaining.
But apparently there is some research backing this up and
[Im not the only one to feel his way](http://paulgraham.com/makersschedule.html).

## Mood and System 1

Some research is mentioned linking the effects of bad mood and System 1.
When you are in a bad mood your system 1 is impaired. That is why it is hard to
be creative when you are in a bad mood, have ideas, make associations, etc.

System 2 is much less affected, so you can still perform more analytical activities.

## When Intuition Matters

I'm always interested on anything that is related to intuition. At first glance it seems like
guessing or some other vague thing like "gut feeling", and yet when you see what experienced
people can do with it, it is remarkable.

But it is important to understand how good intuition works and when it matters.
Well, what would be bad intuition ? That is when you don't have any experience with a problem
or the environment/context of the problem and follow your "gut" anyway, that is essentially
a wild guess and if it goes well will be by sheer luck.

Intuition is not magic. Intuition is part of the System 1 (hence why it is fast) but it must
be grounded on two pillars:

* Deliberate Practice (not just repeating the same thing)
* Stable Environment

The caveat is the stable environment. If the environment is extremelly dynamic/chaotic, even
deliberate practice won't help you. The way intuition works is by pattern detection from your
previous experiences, for it to be meaningful there must be a pattern to detect, and part of
that is the environment. If the environment changes in unpredictable ways experience won't help
you that much, including previous success. There must be some degree of predictability on the 
environment for expert knowledge to be valuable.

That makes a lot of sense, I work in computing and it is remarkable how a lot of really smart
people, with lots of experience and success, made the wildest/most stupid predictions about the
future of computing. It is not that they are stupid, it is just that being smart and having
experience won't help you to predict the future in this case, because it is unpredictable by
definition.

Of course that in more short term things, and building actual software, experience helps a lot,
in the end there are a bunch of patterns on how people behave and structuers in software, so
experience will be invaluable. But that doesn't transport to the overall future of computing,
no one knows. Intuition can't handle unpredictability, it is not magic.

## Memory: The Two Selves

On the topic of remembering we have 2 selves, the remembering self and the experiencing self.
The experiencing self is the one that actually went through the experience (objective reality),
the remembering self is the one that remembers the experience, and guess what, they are totally different.

The way we remember experienced events is by sampling the spike of the experience and the
end and creating an average of both (a form of compression happens in the brain).

An implication of this is that we remember things that ended badly MUCH worse than they
actually were. The same goes for people going back together because they are remembering
some recent good experience instead of objectively assessing the total experience.

There are a lot of experiments around this that are quite interesting. One very simple is about
imersing your hand on unconfortably hot water. People were subjected to 2 experiences:

1 - Hand imersed on 14c water for 60 seconds
2 - Hand imersed on 14c water for 60 seconds, then 30 seconds at 15c

Most people on the experiment thought the second experience was better.
The overall experience was obviously worse, but since the second experience
ended better the brain remembered as a more pleasant overall experience.

At the same time that we want to optimize pleasure and avoid pain, our brains
don't help us remember any of this properly, existing is hard.

## Avoiding The Cognitive Minefield

It is really hard to avoid this minefield when you are directly involved, the last
thing you need in a stressful situation is doubt. It is much easier to see a minefield
when you are just an observer, the cognitive load is smaller and you will be more open
to new information.

I have a similar personal experience with this. At least at work when Im not directly
involved in a problem and just want to help I can see things more broadly, see more
options, etc. When Im the one directly involved is like focus makes you see more details
but less context and alternatives. Focus is essential but it is a trade-off, it narrows
your cognitiion, which is good for attention to detail but bad to avoid cognitive pitfalls
since you can't dissociate and analyze the broader picture.

What I usually try to do is to shift between these two modes, but when you are directly
involved in a stressful problem it is really hard to do that. That is why it usually helps
to bring some new perspective from someone not involved directly (or doing some rubber ducking,
these days done with some AI chatbot I suppose =P).

## Biases

Now a list of the biases mentioned throughout the book.

### Consistency and simplicity over accuracy

Our system 1 is predominantly driven by consistency and simplicity in the story
it tells about ourselves and the world. It doesn't enjoy [cognitive dissonance](https://en.wikipedia.org/wiki/Cognitive_dissonance),
even though exposing yourself to it constantly is essential to build critical
thinking.

What does this mean ? It means that when exposed to new information your mind will
prioritize information that already fits on your current mental model of things,
because that is comfortable and makes it easy to create a simple and consistent
narrative about things. If the information is real/accurate is completely irrelevant
to this process.

That is why it is so common for people being exposed to a single side of an argument
to be more sure about their conclusions than people exposed to both. If the
mental model is cohesive it feels strong, even if it is all based on lies
or senseless logic.

Since we are talking about systems this reminded me of the great article
[Worse Is Better](https://www.jwz.org/doc/worse-is-better.html). When explaining the
differences between different design philosophies the author presents 4 main
concepts important when designing systems (or building mental models in this case):

* Simplicity
* Correctness
* Consistency
* Completeness

Both in life and software, you can't have it all, you constantly need to make
tradeoffs on which one will you optimize. Specially when you think about reality,
modeling the world, interactions between people, countries, climate, etc, it is my
opinion that simplicity just flew through the window, reality is not simple.

But then the bad news. Given our limitations as humans we are actually pretty fond
of simplicity. According to research mentioned in the book, specially our
system 1/intuition, is extremelly prone to focusing entirely on simplicity and
consistency. It doesn't matter if the information is correct and it doesn't
matter if you have all information.

One of the biases mentioned on the book is WYSIATI, which stands for What You
See Is All There Is. It reinforces how we always optimize for recent/available
information, hence why we hardly really try to obtain a complete view of
something before coming to a conclusion about it.

My empirical life experience agrees a lot with all this, by observing other
people behavior, and sometimes my own. But maybe I'm just a victim of my
own WYSIATI :-).

### Affect Heuristics

Even our logical/rational System 2 tends to look for information that confirms
System 1 assumptions. I feel this clearly in my life, it is that nice pleasure
you feel when you find information that reinforces something you believe and
that is important for you. But that is also how information bubbles gets formed.

### Anchoring Effect

Suggesting an initial value/information affects your judgement. Even professionals
with a lot of experience get influenced by almost random anchors. Research interviewed
real state agents about the pricing of some real state but providing anchors on
the value of the properties. The anchoring effect was very similar to what
is observed in people who are not giving a professional opinion. So even having
a lot of experience with something won't make you immune to this.
Funny enough the interviewed people denied being influenced by this bias, even
though the results were quite clear.

### Availability Bias

How easily/fast information can be retrieved from our brains affects our confidence
on the information and reinforces our belief in it. This seems ok/good at first,
but leads to some paradoxical results.

In one of the researches around this people were asked to remember of 12 times
they were assertive, and they asked how assertive they think they are. The same
was done with a different set of people, but asking for just 6 times.

Logically, people who remembered fewer ocassions would feel less confident.
But the opposite happened, people who remembered only 6 occasions were more confident
than the ones who remember 12.

Remembering 12 occasions takes longer and is harder. When it is hard to remember
something you develop the intuition that it is less likely to be true, even
if in the end you gathered even more information.

This explains my personal bias against assertive people. Usually the people with
less information/experience are the more assertive ones.

### Affect Heuristic

We tend to not assess tradeoffs between different ideas/things, we just push for simpler
and black/white models of reality. What we like tend to have only benefits and
mostly no downsides, and what we don't like has only downsides, no benefits at all.

This explains a lot about human nature and why almost any form of human endeavour
ends up converging to religious behavior. Religion is usually built on top of
this bias, the affinity that we have with simple answers, right and wrong,
good and evil.

It doesn't model reality, but our brains are not that interested in reality :-)
(just the bare minimum to keep us alive and confortable).

### Loss Aversion

We are biased towards having more aversion to loss than to the benefits of gains.
This comes from the fact that we pay more attention and learn faster from bad experiences.
In nature it is more important to be able to identify predators and dangerous environments.
You need to learn where you can gain things, like food, but knowing where danger is fundamental,
you can be wrong multiple times about food and spend days without eating, you are going to be
wrong about a predator only once.

An interesting case of loss aversion is the behavior of most taxi drivers.
They have a lot of work on rainy days. Economically/rationally it makes much more
sense to work a lot on rainy days and take the sunny days off.

But the general behavior is irrational. They work less on rainy days, because it takes
a shorter time to make the daily quota and work extra hard on sunny days just so they can
reach a quota. This is loss aversion, you don't want to "lose" the sunny day, but it makes
no sense, if you work much more on the rainy day and not at all on the sunny day you would make
more money overall.

### Denominator Bias

This one is quite funny. When asked about two choices:

1 - 8/100 balls
2 - 1/10 balls

40% of the interviewees chose 8/100 balls, even though it clearly
is less balls than 1/10. Why ? we tend to create a more positve image
when presented with more (8 balls versus a single one). It is classic
irrational behavir in place, anyone that stops to do the actual math
(system 2 reasoning) will clearly see the best option.

### Framing Effect

How we frame a question changes our emotional response to it and decision
making (if you decide emotionally, as most people do).

Examples:

- When credit cards started, instead of talking about credit card surcharge people
started talking about discounts when paying with money (same thing in the end).

- Talk about survival rates instead of mortality rates

When people are more objective about this differences can be seen on their brains
(using MRI).
